{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dspy methodology 101\n",
    "\n",
    "1. programming\n",
    "   1. LMs (tasks)\n",
    "   2. signatures (i/o - eg `\"context: list[str], question: str -> answer: str\"`) - compiling leads to better prompts than humans write\n",
    "      1. tasks, instruct the model what it needs to do\n",
    "      2. underlying dSPY compiler will do the optimization, rather than brittle prompts\n",
    "   3. modules (ie `dspy.Predict`, `dspy.ChainOfThought`)\n",
    "      1. prompting techniques\n",
    "2. evaluation\n",
    "3. optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [intro](#dspy-methodology-101)\n",
    "* [LMs](#set-a-generator-lm)\n",
    "* [evaluations 101](#dspy-evaluations)\n",
    "* [data](#dspy-data)\n",
    "  * [example-obects](#dspy-example-objects)\n",
    "* [metrics](#dspy-metrics)\n",
    "* [evaluations](#dspy-evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set a generator LM\n",
    "\n",
    "<a class=\"anchor\" id=\"LM\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "\n",
    "lm=dspy.LM('together_ai/deepseek-ai/DeepSeek-R1', temperature=0.1, max_tokens=2500, stop=None, cache=False, api_key=TOGETHER_API_KEY)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dSPY evaluations\n",
    "\n",
    "- define your DSPy metric\n",
    "  - what makes outputs from your system good or bad?\n",
    "    - A metric is a function that takes examples from your data and takes the output of your system, and returns a score.\n",
    "    - you use Examples a lot in DSPy to train, test, and improve AI models\n",
    "- no labels, just inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Data\n",
    "\n",
    "- inputs\n",
    "- intermediate labels \n",
    "- final label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DSPy Example objects\n",
    "\n",
    "- Examples represent items in your training set and test set\n",
    "- Examples = AI-friendly data containers\n",
    "\n",
    "**ML**:\n",
    "\n",
    "- the Example objects have a `with_inputs()` method, which can mark specific fields as inputs. (The rest are just metadata or [labels](https://toloka.ai/blog/machine-learning-labels-and-features/) - A label is a description that informs an ML model what a particular data represents so that it may learn from the example)\n",
    "\n",
    "- Inputs (Features) → The data you give to the model to make a prediction.\n",
    "  - **Example**: A picture of a cat, a sentence, or a set of numbers.\n",
    "- Labels (Targets/Outputs) → The correct answer the model should learn to predict.\n",
    "  - **Example**: The word \"cat\" for an image classification model, or the correct sentiment (positive/negative) for a text review.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "If you're training a spam detector:\n",
    "**Input**: An email's text\n",
    "**Label**: \"Spam\" or \"Not Spam\"\n",
    "\n",
    "```bash\n",
    "# Single Input.\n",
    "print(qa_pair.with_inputs(\"question\"))\n",
    "\n",
    "# Multiple Inputs; be careful about marking your labels as inputs unless you mean it.\n",
    "print(qa_pair.with_inputs(\"question\", \"answer\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': 'This is a question?', 'answer': 'This is an answer.'}) (input_keys=None)\n",
      "This is a question?\n",
      "This is an answer.\n"
     ]
    }
   ],
   "source": [
    "qa_pair = dspy.Example(question=\"This is a question?\", answer=\"This is an answer.\")\n",
    "\n",
    "print(qa_pair)\n",
    "print(qa_pair.question)\n",
    "print(qa_pair.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': 'This is a question?', 'answer': 'This is an answer.'}) (input_keys={'question'})\n",
      "Example({'question': 'This is a question?', 'answer': 'This is an answer.'}) (input_keys={'answer', 'question'})\n"
     ]
    }
   ],
   "source": [
    "# Single Input.\n",
    "print(qa_pair.with_inputs(\"question\"))\n",
    "\n",
    "# Multiple Inputs; be careful about marking your labels as inputs unless you mean it.\n",
    "print(qa_pair.with_inputs(\"question\", \"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example object with Input fields only: Example({'article': 'This is an article.'}) (input_keys={'article'})\n",
      "Example object with Non-Input fields only: Example({'summary': 'This is a summary.'}) (input_keys=None)\n"
     ]
    }
   ],
   "source": [
    "article_summary = dspy.Example(article= \"This is an article.\", summary= \"This is a summary.\").with_inputs(\"article\")\n",
    "\n",
    "input_key_only = article_summary.inputs()\n",
    "non_input_key_only = article_summary.labels()\n",
    "\n",
    "print(\"Example object with Input fields only:\", input_key_only)\n",
    "print(\"Example object with Non-Input fields only:\", non_input_key_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example object with Input fields only: Example({'article': 'This is an article.'}) (input_keys={'article'})\n",
      "Example object with Non-Input fields only: Example({'summary': 'This is a summary.'}) (input_keys=None)\n"
     ]
    }
   ],
   "source": [
    "article_summary = dspy.Example(article= \"This is an article.\",\n",
    "                               summary= \"This is a summary.\").with_inputs(\"article\")\n",
    "\n",
    "input_key_only = article_summary.inputs()\n",
    "non_input_key_only = article_summary.labels()\n",
    "\n",
    "print(\"Example object with Input fields only:\", input_key_only)\n",
    "print(\"Example object with Non-Input fields only:\", non_input_key_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dSPY metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A metric is just a function that will take examples from your data and the output of your system and return a score that quantifies how good the output is\n",
    "\n",
    "AKA good vs bad outputs\n",
    "\n",
    "for simple tasks, this could be just \n",
    "  - \"accuracy\"\n",
    "  - \"exact match\"\n",
    "  - \"F1 score\" (precision and re-call)\n",
    "    - 1 (or 100%) → Perfect model\n",
    "    - 0 → Totally useless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dspy_metric(example, pred):\n",
    "    \"\"\"\n",
    "    A DSPy metric function.\n",
    "\n",
    "    Parameters:\n",
    "    - example: An example from your training or dev set.\n",
    "    - pred: The output prediction from your DSPy program.\n",
    "\n",
    "    Returns:\n",
    "    - score: A float, int, or bool score.\n",
    "    \"\"\"\n",
    "    # Your metric calculation logic here\n",
    "    score = calculate_score(example, pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`trace` is the third argument and can be used to optimize the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_answer(example, pred, trace=None):\n",
    "    return example.answer.lower() == pred.answer.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=center>during compiling (optimization), DSPy will trace your LM calls. The trace will contain inputs/outputs to each DSPy predictor and you can leverage that to validate intermediate steps for optimization.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_hops(example, pred, trace=None):\n",
    "    hops = [example.question] + [outputs.query for *_, outputs in trace if 'query' in outputs]\n",
    "\n",
    "    if max([len(h) for h in hops]) > 100: return False\n",
    "    if any(dspy.evaluate.answer_exact_match_str(hops[idx], hops[:idx], frac=0.8) for idx in range(2, len(hops))): return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a couple of example built-in common metrics:\n",
    "- `dspy.evaluate.metrics.answer_exact_match`\n",
    "- `dspy.evaluate.metrics.answer_passage_match`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for x in devset:\n",
    "    pred = program(**x.inputs())\n",
    "    score = metric(x, pred)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the built-in Evaluate utility can help with things like parallel evaluation (multiple threads) or showing you a sample of inputs/outputs and the metric scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "# Set up the evaluator, which can be re-used in your code.\n",
    "evaluator = Evaluate(devset=YOUR_DEVSET, num_threads=1, display_progress=True, display_table=5)\n",
    "\n",
    "# Launch evaluation.\n",
    "evaluator(YOUR_PROGRAM, metric=YOUR_METRIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(gold, pred, trace=None):\n",
    "    question, answer, tweet = gold.question, gold.answer, pred.output\n",
    "\n",
    "    engaging = \"Does the assessed text make for a self-contained, engaging tweet?\"\n",
    "    correct = f\"The text should answer `{question}` with `{answer}`. Does the assessed text contain this answer?\"\n",
    "\n",
    "    correct =  dspy.Predict(Assess)(assessed_text=tweet, assessment_question=correct)\n",
    "    engaging = dspy.Predict(Assess)(assessed_text=tweet, assessment_question=engaging)\n",
    "\n",
    "    correct, engaging = [m.assessment_answer for m in [correct, engaging]]\n",
    "    score = (correct + engaging) if correct and (len(tweet) <= 280) else 0\n",
    "\n",
    "    if trace is not None: return score >= 2\n",
    "    return score / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=center>When compiling, trace is not None, and we want to be strict about judging things, so we will only return True if score >= 2. Otherwise, we return a score out of 1.0 (i.e., score / 2.0).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If your metric is itself a DSPy program, one of the most powerful ways to iterate is to compile (optimize) your metric itself. That's usually easy because the output of the metric is usually a simple value (e.g., a score out of 5) so the metric's metric is easy to define and optimize by collecting a few examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
